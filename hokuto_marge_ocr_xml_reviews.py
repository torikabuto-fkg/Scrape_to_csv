#!/usr/bin/env python3
"""
OCRãƒ‡ãƒ¼ã‚¿ã¨XMLæŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ã—ã¦é«˜ç²¾åº¦ãªãƒ¬ãƒ“ãƒ¥ãƒ¼æŠ½å‡ºã‚’è¡Œã†

OCRãƒ‡ãƒ¼ã‚¿: ã€Œè‰¯ã„ç‚¹ã€ã€Œæ°—ã«ãªã‚‹ç‚¹ã€ã®è¦‹å‡ºã—ã‚’å«ã‚€
XMLãƒ‡ãƒ¼ã‚¿: æœ¬æ–‡ãŒæ­£ç¢ºã ãŒè¦‹å‡ºã—ãŒæ¬ è½ã—ã¦ã„ã‚‹

ä¸¡æ–¹ã‚’ãƒãƒƒãƒãƒ³ã‚°ã—ã¦ã€XMLã®æœ¬æ–‡ã«OCRã®è¦‹å‡ºã—ã‚’è¿½åŠ ã™ã‚‹
"""

import json
import re
import sys
from matplotlib import lines
from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment
from difflib import SequenceMatcher

# ===== ãƒã‚¤ã‚ºé™¤å»ã®è¨­å®šï¼ˆæ£æ„çš„ã«ON/OFFã§ãã‚‹ï¼‰=====
TRUNCATE_AFTER_REPORT_LINE = True   # ã€Œãƒ­ã‚³ãƒŸ/å£ã‚³ãƒŸã®å•é¡Œã‚’å ±å‘Šã€ã‚’è¦‹ã¤ã‘ãŸã‚‰ä»¥é™ã‚’æ¨ã¦ã‚‹
REMOVE_MD_HEADINGS = True           # ## è‰¯ã„ç‚¹ / ## æ°—ã«ãªã‚‹ç‚¹ ã‚’æ¶ˆã™

REPORT_LINE_RE = re.compile(r'(ãƒ­ã‚³ãƒŸ|å£ã‚³ãƒŸ)ã®å•é¡Œã‚’å ±å‘Š')

# ã€Œã“ã®è¡Œã¯æœ¬æ–‡ã§ã¯ãªã„ã€æ‰±ã„ã§è¡Œã”ã¨æ¶ˆã™ãƒ‘ã‚¿ãƒ¼ãƒ³
NOISE_LINE_RES = [
    re.compile(r'^\s*ã‚‚ã£ã¨è¦‹ã‚‹\s*$'),
    re.compile(r'^\s*å…¨ã¦ã®å£ã‚³ãƒŸã‚’èª­ã‚€.*$'),
    re.compile(r'^\s*å£ã‚³ãƒŸã®å•é¡Œã‚’å ±å‘Š.*$'),
    re.compile(r'^\s*ãƒ­ã‚³ãƒŸã®å•é¡Œã‚’å ±å‘Š.*$'),

    # ç”»åƒ/ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚«ãƒ¼ç³»ï¼ˆè¡Œã ã‘æ¶ˆã™ï¼šã“ã“ã§æœ¬æ–‡ã‚’åˆ‡ã‚‰ãªã„ï¼‰
    re.compile(r'^\s*=+\s*.*\s*=+\s*$'),
    re.compile(r'.*review_page_.*\.png.*', re.IGNORECASE),
    re.compile(r'.*\.png.*', re.IGNORECASE),

    # åŒºåˆ‡ã‚Šç·š
    re.compile(r'^\s*-{3,}\s*$'),

    # ã„ã‚ã‚†ã‚‹è¦‹å‡ºã—ã ã‘ã®è¡Œï¼ˆæœ¬æ–‡ã§ã¯ãªã„ï¼‰
    re.compile(r'^\s*#+\s*$'),
]

# ã€Œ## ã‚ã‚Š/ãªã—ã€ã€Œ: / ï¼šã‚ã‚Šã€ãªã©ã®è¦‹å‡ºã—â€œå˜ç‹¬è¡Œâ€ã‚’å…¨éƒ¨æ¶ˆã™
MD_HEADING_RE = re.compile(r'^\s*#{1,6}\s*(è‰¯ã„ç‚¹|æ°—ã«ãªã‚‹ç‚¹)\s*$', re.UNICODE)


def clean_review_text(text: str) -> str:
    """
    ãƒ¬ãƒ“ãƒ¥ãƒ¼æœ¬æ–‡ã‹ã‚‰ãƒã‚¤ã‚ºã‚’é™¤å»ã€‚
    - è¡Œå˜ä½ã§ãƒã‚¤ã‚ºã‚’è½ã¨ã™
    - ã€Œãƒ­ã‚³ãƒŸ/å£ã‚³ãƒŸã®å•é¡Œã‚’å ±å‘Šã€è¡ŒãŒå‡ºãŸã‚‰ã€ãã®ãƒ¬ãƒ“ãƒ¥ãƒ¼å†…ã®æ®‹ã‚Šã¯æ¨ã¦ã‚‹ï¼ˆè¨­å®šã§ON/OFFï¼‰
    - Markdownè¦‹å‡ºã—ï¼ˆ## è‰¯ã„ç‚¹ ç­‰ï¼‰ã‚‚è½ã¨ã™ï¼ˆè¨­å®šã§ON/OFFï¼‰
    """
    if not text or not isinstance(text, str):
        return ''

    text = text.replace('\r\n', '\n').replace('\r', '\n')

    cleaned_lines = []
    for raw_line in text.split('\n'):
        line = raw_line.strip()
        if not line:
            continue

        # ã“ã“ã«æ¥ãŸã‚‰ä»¥é™ã¯æ¨ã¦ã‚‹ï¼ˆâ€œæ¬¡ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¾ã§â€ã¯ãƒ‘ãƒ¼ã‚µå´ã§å‡¦ç†ã™ã‚‹ãŒã€ã‚»ãƒ«å†…ã¯ã“ã‚Œã§OKï¼‰
        if TRUNCATE_AFTER_REPORT_LINE and REPORT_LINE_RE.search(line):
            break

        # Markdownè¦‹å‡ºã—ã‚’æ¶ˆã™
        if REMOVE_MD_HEADINGS and MD_HEADING_RE.match(line):
            continue

        # è¡Œãƒã‚¤ã‚ºåˆ¤å®š
        is_noise = False
        for rex in NOISE_LINE_RES:
            if rex.match(line):
                is_noise = True
                break
        if is_noise:
            continue

        cleaned_lines.append(line)

    # é€£ç¶šç©ºè¡Œã®æ­£è¦åŒ–ï¼ˆã“ã“ã§ã¯ç©ºè¡Œã‚’å…¥ã‚Œã¦ãªã„ã®ã§ä¸è¦ã ãŒä¸€å¿œï¼‰
    out = '\n'.join(cleaned_lines).strip()
    return out

STRICT_UNIVERSITY_FILTER = True  # Trueãªã‚‰ã€Œå¤§å­¦åã£ã½ããªã„ã‚‚ã®ã€ã¯ç©ºæ¬„ã«ã™ã‚‹

UNIV_DENY_KEYWORDS = [
    'å£ã‚³ãƒŸ', 'ãƒ­ã‚³ãƒŸ', 'å•é¡Œã‚’å ±å‘Š', 'ã‚‚ã£ã¨è¦‹ã‚‹', 'å…¨ã¦ã®å£ã‚³ãƒŸ', 'ç—…é™¢æƒ…å ±', 'ç—…é™¢ãƒ»ç ”ä¿®',
    'review_page_', '.png', 'æ¡ç”¨ã§', 'p)', 'è¦‹å­¦ã—ãŸ', 'ãƒãƒƒãƒã—ãŸ', 'å¹´åº¦', 'ç”·æ€§', 'å¥³æ€§'
]

# ã€Œå¤§å­¦åã‚‰ã—ã•ã€ã®æœ€ä½æ¡ä»¶ï¼ˆå¿…è¦ãªã‚‰è¿½åŠ ã—ã¦OKï¼‰
UNIV_ALLOW_RE = re.compile(r'(å¤§å­¦|åŒ»ç§‘å¤§å­¦|åŒ»ç§‘æ­¯ç§‘å¤§å­¦|å¤§å­¦æ ¡|åŒ»å­¦éƒ¨|åŒ»å¤§)')

# æ–‡å­—åˆ—ä¸­ã‹ã‚‰å¤§å­¦åã£ã½ã„éƒ¨åˆ†ã ã‘æŠœãå‡ºã™ï¼ˆæœ€å¾Œã«å‡ºãŸå€™è£œã‚’æ¡ç”¨ï¼‰
UNIV_EXTRACT_RE = re.compile(
    r'([^\sã€€]{2,60}?(?:åŒ»ç§‘æ­¯ç§‘å¤§å­¦|åŒ»ç§‘å¤§å­¦|å¤§å­¦æ ¡|å¤§å­¦|åŒ»å¤§)(?:åŒ»å­¦éƒ¨)?)'
)

def normalize_university_name(raw: str) -> str:
    """å¤§å­¦åä»¥å¤–ã£ã½ã„ã‚‚ã®ã‚’é™¤å»ã—ã¦ã€å¤§å­¦åå€™è£œã ã‘è¿”ã™ï¼ˆãªã‘ã‚Œã°ç©ºæ¬„ï¼‰"""
    if not raw or not isinstance(raw, str):
        return ''

    # ã–ã£ãã‚Šãƒã‚¤ã‚ºåˆ¤å®šï¼ˆå…ƒã®æ–‡å­—åˆ—ã«ãƒã‚¤ã‚ºãŒæ··ã–ã£ã¦ãŸã‚‰è½ã¨ã™ï¼‰
    if any(k in raw for k in UNIV_DENY_KEYWORDS):
        return ''

    # OCRã®ä½™è¨ˆãªç©ºç™½ã‚’æ½°ã—ã¦ã‹ã‚‰å€™è£œæŠ½å‡º
    compact = re.sub(r'\s+', '', raw)

    # ã€Œå¤§å­¦/åŒ»å¤§/åŒ»å­¦éƒ¨ã€ç­‰ãŒå«ã¾ã‚Œãªã„ãªã‚‰å¤§å­¦åã§ã¯ãªã„æ‰±ã„
    if not UNIV_ALLOW_RE.search(compact):
        return '' if STRICT_UNIVERSITY_FILTER else raw.strip()

    candidates = UNIV_EXTRACT_RE.findall(compact)
    if not candidates:
        return '' if STRICT_UNIVERSITY_FILTER else raw.strip()

    cand = candidates[-1].strip()

    # ã€Œå¤§å­¦ã€ã ã‘ã¿ãŸã„ãªå¼±ã™ãã‚‹å€™è£œã¯è½ã¨ã™
    if cand in ('å¤§å­¦', 'åŒ»å¤§', 'åŒ»å­¦éƒ¨'):
        return ''

    # é•·ã™ãã‚‹ã®ã‚‚ãƒã‚¤ã‚ºæ‰±ã„ï¼ˆå¿…è¦ãªã‚‰é–¾å€¤èª¿æ•´ï¼‰
    if len(cand) > 40:
        return ''

    return cand

# ===== åˆ†è£‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®çµåˆè¨­å®š =====
MERGE_ADJACENT_OVERLAP = True
MIN_OVERLAP_NORM_CHARS = 40      # é‡è¤‡ã¨ã¿ãªã™æœ€å°ä¸€è‡´é•·ï¼ˆç©ºç™½é™¤å»å¾Œã®æ–‡å­—æ•°ï¼‰
MAX_OVERLAP_NORM_CHARS = 300     # æ¢ã™é‡è¤‡é•·ã®ä¸Šé™ï¼ˆé€Ÿåº¦å¯¾ç­–ï¼‰
FUZZY_OVERLAP_RATIO = 0.93       # å®Œå…¨ä¸€è‡´ã—ãªã„å ´åˆã®è¨±å®¹ï¼ˆOCRèª¤å·®ç”¨ï¼‰
HIGH_SIM_RATIO = 0.92            # ã»ã¼åŒã˜æ–‡ç« ã®ã¨ãã¯é•·ã„æ–¹ã‚’æ¡ç”¨


def _norm_no_ws(s: str) -> str:
    return re.sub(r'\s+', '', s or '')


def _index_after_norm_chars(original: str, norm_chars: int) -> int:
    """originalã®å…ˆé ­ã‹ã‚‰ã€ç©ºç™½é™¤å»å¾Œã®æ–‡å­—ã‚’norm_charsåˆ†é€²ã‚ãŸä½ç½®(ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)ã‚’è¿”ã™"""
    if norm_chars <= 0:
        return 0
    cnt = 0
    for i, ch in enumerate(original):
        if not ch.isspace():
            cnt += 1
            if cnt >= norm_chars:
                return i + 1
    return len(original)


def _best_overlap_len(a_norm: str, b_norm: str,
                      min_len: int, max_len: int, fuzzy_ratio: float) -> int:
    """
    a_normã®suffix ã¨ b_normã®prefix ã®é‡ãªã‚Šé•·ã‚’è¿”ã™ï¼ˆç©ºç™½é™¤å»æ¸ˆã¿å‰æï¼‰
    å®Œå…¨ä¸€è‡´ãŒç„¡ã‘ã‚Œã°ã€fuzzy_ratioä»¥ä¸Šãªã‚‰è¨±å®¹
    """
    limit = min(len(a_norm), len(b_norm), max_len)
    if limit < min_len:
        return 0

    for l in range(limit, min_len - 1, -1):
        s1 = a_norm[-l:]
        s2 = b_norm[:l]
        if s1 == s2:
            return l
        # OCRèª¤å·®ã‚’è¨±å®¹ã—ãŸã„å ´åˆã®ã¿
        if SequenceMatcher(None, s1, s2).ratio() >= fuzzy_ratio:
            return l
    return 0


def merge_text_by_overlap(a: str, b: str,
                          min_overlap: int = MIN_OVERLAP_NORM_CHARS,
                          max_overlap: int = MAX_OVERLAP_NORM_CHARS,
                          fuzzy_ratio: float = FUZZY_OVERLAP_RATIO):
    """
    a ã¨ b ãŒé‡è¤‡ã—ã¦ã„ã‚‹ãªã‚‰é‡è¤‡éƒ¨åˆ†ã‚’ã‚«ãƒƒãƒˆã—ã¦é€£çµã—ãŸæ–‡å­—åˆ—ã‚’è¿”ã™ã€‚
    æˆ»ã‚Šå€¤: (merged_text, merged_bool)
    """
    a = a or ''
    b = b or ''
    a_norm = _norm_no_ws(a)
    b_norm = _norm_no_ws(b)

    if not a_norm:
        return b, True
    if not b_norm:
        return a, True

    # ç‰‡æ–¹ãŒå®Œå…¨ã«å«ã¾ã‚Œã‚‹ â†’ é•·ã„æ–¹ã ã‘æ®‹ã™
    if a_norm in b_norm:
        return b, True
    if b_norm in a_norm:
        return a, True

    # æ–¹å‘1: aã®æœ«å°¾ã¨bã®å…ˆé ­ãŒé‡ãªã‚‹
    l1 = _best_overlap_len(a_norm, b_norm, min_overlap, max_overlap, fuzzy_ratio)
    # æ–¹å‘2: bã®æœ«å°¾ã¨aã®å…ˆé ­ãŒé‡ãªã‚‹ï¼ˆé †åºãŒé€†ã ã£ãŸå ´åˆã®æ•‘æ¸ˆï¼‰
    l2 = _best_overlap_len(b_norm, a_norm, min_overlap, max_overlap, fuzzy_ratio)

    if l1 == 0 and l2 == 0:
        return a, False

    # ã‚ˆã‚Šå¤§ãã„é‡ãªã‚Šã‚’æ¡ç”¨
    if l1 >= l2:
        cut = _index_after_norm_chars(b, l1)
        suffix = b[cut:].lstrip()
        joiner = '\n' if (a and not a.endswith('\n') and suffix) else ''
        return (a.rstrip() + joiner + suffix), True
    else:
        cut = _index_after_norm_chars(a, l2)
        suffix = a[cut:].lstrip()
        joiner = '\n' if (b and not b.endswith('\n') and suffix) else ''
        return (b.rstrip() + joiner + suffix), True


def _same_meta_for_adjacent_merge(r1: dict, r2: dict) -> bool:
    """éš£åŒå£«ã‚’ãƒãƒ¼ã‚¸ã—ã¦ã‚ˆã„ã‹ã®æœ€ä½æ¡ä»¶ï¼ˆèª¤çµåˆé˜²æ­¢ï¼‰"""
    if r1.get('year') != r2.get('year'):
        return False
    if r1.get('grade') != r2.get('grade'):
        return False
    if r1.get('participation') != r2.get('participation'):
        return False

    # æ€§åˆ¥ãŒä¸¡æ–¹åŸ‹ã¾ã£ã¦ã„ã¦é•ã†ãªã‚‰åˆ¥äººæ‰±ã„
    g1 = (r1.get('gender') or '').strip()
    g2 = (r2.get('gender') or '').strip()
    if g1 and g2 and g1 != g2:
        return False

    # å¤§å­¦åãŒä¸¡æ–¹åŸ‹ã¾ã£ã¦ã„ã¦é•ã†ãªã‚‰åˆ¥äººæ‰±ã„ï¼ˆâ€»ç©ºæ¬„ã¯è¨±å®¹ï¼‰
    u1 = (r1.get('university') or '').strip()
    u2 = (r2.get('university') or '').strip()
    if u1 and u2 and u1 != u2:
        return False

    return True


def merge_adjacent_overlapping_reviews(reviews: list[dict]) -> list[dict]:
    """
    OCRç”±æ¥ã®ã€Œå‰å¾Œã§åˆ†è£‚ã—ãŸãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ã‚’çµåˆã™ã‚‹ã€‚
    - éš£ã®è¡ŒåŒå£«ã ã‘è¦‹ã‚‹ï¼ˆèª¤çµåˆã‚’æ¸›ã‚‰ã™ï¼‰
    - good_pointsåŒå£«ã€concernsåŒå£«ã®é‡ãªã‚Šã ã‘çµåˆ
    - ã»ã¼åŒã˜æ–‡ç« ãªã‚‰é•·ã„æ–¹ã‚’æ¡ç”¨
    """
    if not reviews:
        return reviews

    out = []
    i = 0
    while i < len(reviews):
        cur = dict(reviews[i])

        j = i + 1
        while j < len(reviews):
            nxt = reviews[j]

            if not _same_meta_for_adjacent_merge(cur, nxt):
                break

            changed = False

            # good_points ã®é‡ãªã‚Š
            if (cur.get('good_points') and nxt.get('good_points')):
                merged, ok = merge_text_by_overlap(cur['good_points'], nxt['good_points'])
                if ok:
                    cur['good_points'] = merged
                    changed = True
                else:
                    # ã»ã¼åŒã˜ãªã‚‰é•·ã„æ–¹ã ã‘æ®‹ã™
                    if text_similarity(cur['good_points'], nxt['good_points']) >= HIGH_SIM_RATIO:
                        if len(_norm_no_ws(nxt['good_points'])) > len(_norm_no_ws(cur['good_points'])):
                            cur['good_points'] = nxt['good_points']
                        changed = True

            # concerns ã®é‡ãªã‚Š
            if (cur.get('concerns') and nxt.get('concerns')):
                merged, ok = merge_text_by_overlap(cur['concerns'], nxt['concerns'])
                if ok:
                    cur['concerns'] = merged
                    changed = True
                else:
                    if text_similarity(cur['concerns'], nxt['concerns']) >= HIGH_SIM_RATIO:
                        if len(_norm_no_ws(nxt['concerns'])) > len(_norm_no_ws(cur['concerns'])):
                            cur['concerns'] = nxt['concerns']
                        changed = True

            # ã©ã¡ã‚‰ã‚‚çµåˆã§ããªã„ãªã‚‰ã“ã“ã§çµ‚äº†ï¼ˆéš£ä»¥å¤–ã¾ã§ã¯è¿½ã‚ãªã„ï¼‰
            if not changed:
                break

            # ãƒ¡ã‚¿æƒ…å ±ã®ç©´åŸ‹ã‚ï¼ˆç©ºæ¬„å„ªå…ˆã§åŸ‹ã‚ã‚‹ï¼‰
            if not (cur.get('university') or '').strip():
                cur['university'] = nxt.get('university', '')
            if not (cur.get('gender') or '').strip():
                cur['gender'] = nxt.get('gender', '')

            j += 1

        out.append(cur)
        i = j

    return out

EXPLICIT_HEAD_RE = re.compile(r'(?m)^\s*(?:#{1,6}\s*)?(è‰¯ã„ç‚¹|æ°—ã«ãªã‚‹ç‚¹)\s*(?:[:ï¼š]\s*)?$', re.UNICODE)

def split_by_explicit_headings(text: str):
    if not text:
        return None
    t = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
    hits = list(EXPLICIT_HEAD_RE.finditer(t))
    if not hits:
        return None

    # 1å€‹ã ã‘ãªã‚‰ã€ãã®è¦‹å‡ºã—ä»¥é™ã‚’å…¨éƒ¨ãã®å´ã¸
    if len(hits) == 1:
        h = hits[0]
        body = t[h.end():].strip()
        if h.group(1) == 'è‰¯ã„ç‚¹':
            return {'good_points': body, 'concerns': ''}
        else:
            return {'good_points': '', 'concerns': body}

    # è¤‡æ•°ã‚ã‚‹ãªã‚‰ã€æœ€åˆã®2å€‹ã ã‘ã§åˆ†å‰²ï¼ˆé€šå¸¸ã¯è‰¯ã„ç‚¹â†’æ°—ã«ãªã‚‹ç‚¹ï¼‰
    h1, h2 = hits[0], hits[1]
    part1 = t[h1.end():h2.start()].strip()
    part2 = t[h2.end():].strip()

    if h1.group(1) == 'è‰¯ã„ç‚¹' and h2.group(1) == 'æ°—ã«ãªã‚‹ç‚¹':
        return {'good_points': part1, 'concerns': part2}
    if h1.group(1) == 'æ°—ã«ãªã‚‹ç‚¹' and h2.group(1) == 'è‰¯ã„ç‚¹':
        return {'good_points': part2, 'concerns': part1}

    return None

def load_jsonl(file_path):
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data


def extract_hospital_info_xml(lines):
    """XMLæŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç—…é™¢æƒ…å ±ã‚’æŠ½å‡º"""
    info = {}
    info_markers = {
        'ãƒãƒƒãƒè€…æ•°/å®šå“¡': r'ãƒãƒƒãƒè€…æ•°/å®šå“¡\((\d{4})å¹´\)',
        'å¼·ã„ç§‘': r'å¼·ã„ç§‘',
        'ä¸Šç´šåŒ»ã®ä¸»ãªå‡ºèº«å¤§å­¦': r'ä¸Šç´šåŒ»ã®ä¸»ãªå‡ºèº«å¤§å­¦',
        'ç—…åºŠæ•°': r'ç—…åºŠæ•°',
        'çµ¦ä¸': r'çµ¦ä¸',
        'æ•‘æ€¥æŒ‡å®š': r'æ•‘æ€¥æŒ‡å®š',
        'ç—…é™¢è¦‹å­¦æƒ…å ±URL': r'ç—…é™¢è¦‹å­¦æƒ…å ±URL'
    }
    
    for i, line in enumerate(lines[:100]):
        for key, pattern in info_markers.items():
            if re.search(pattern, line):
                if key == 'ãƒãƒƒãƒè€…æ•°/å®šå“¡' and i + 1 < len(lines):
                    info[key] = lines[i + 1].strip()
                elif key in ['å¼·ã„ç§‘', 'ä¸Šç´šåŒ»ã®ä¸»ãªå‡ºèº«å¤§å­¦'] and i + 1 < len(lines):
                    info[key] = lines[i + 1].strip()
                elif key in ['ç—…åºŠæ•°', 'çµ¦ä¸', 'æ•‘æ€¥æŒ‡å®š', 'ç—…é™¢è¦‹å­¦æƒ…å ±URL'] and i + 1 < len(lines):
                    info[key] = lines[i + 1].strip()
    
    return info


def parse_ocr_reviews(ocr_data):
    """OCRãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æŠ½å‡ºï¼ˆè¦‹å‡ºã—åŸºæº–ã§åˆ†å‰²ã€è‰¯ã„ç‚¹ãƒ»æ°—ã«ãªã‚‹ç‚¹ã‚’åˆ¥è¡Œã«ï¼‰"""
    all_sections = []
    
    # å¹´åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³
    year_pattern = r'(?:([^\n]+?)\s+)?(\d+å¹´)\s+(?:(ç”·æ€§|å¥³æ€§)\s+)?(è¦‹å­¦ã—ãŸ|ãƒãƒƒãƒã—ãŸ|ã‚ªãƒ³ãƒ©ã‚¤ãƒ³èª¬æ˜ä¼šã«å‚åŠ ã—ãŸ|èª¬æ˜ä¼šã«å‚åŠ ã—ãŸ)\s+(\d{4}å¹´åº¦)'
    
    # ã¾ãšå…¨ãƒšãƒ¼ã‚¸ã‹ã‚‰è¦‹å‡ºã—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åé›†
    for page_idx, page_data in enumerate(ocr_data):
        text = page_data['text']
        
        # è‰¯ã„ç‚¹ãƒ»æ°—ã«ãªã‚‹ç‚¹ã®è¦‹å‡ºã—ã‚’å…¨ã¦è¦‹ã¤ã‘ã‚‹ï¼ˆ###ä»˜ãã«ã‚‚å¯¾å¿œï¼‰
        heading_pattern = r'(?m)^\s*(?:#{2,3}\s*)?(è‰¯ã„ç‚¹|æ°—ã«ãªã‚‹ç‚¹)\s*(?:[:ï¼š]\s*)?$'
        headings = list(re.finditer(heading_pattern, text))

        for i, heading in enumerate(headings):
            heading_type = heading.group(1)  # "è‰¯ã„ç‚¹" or "æ°—ã«ãªã‚‹ç‚¹"
            
            # ã“ã®è¦‹å‡ºã—ã®å‰ã«ã‚ã‚‹å¹´åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ï¼ˆæœ€å¤§500æ–‡å­—å‰ã¾ã§ï¼‰
            search_start = max(0, heading.start() - 500)
            pre_text = text[search_start:heading.start()]
            
            # æœ€ã‚‚è¿‘ã„å¹´åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹
            year_matches = list(re.finditer(year_pattern, pre_text))
            if not year_matches:
                continue  # å¹´åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            
            last_year_match = year_matches[-1]  # æœ€ã‚‚è¿‘ã„ï¼ˆæœ€å¾Œã®ï¼‰ãƒãƒƒãƒ
            
            university = last_year_match.group(1).strip() if last_year_match.group(1) else ''
            
            # ãƒã‚¤ã‚ºãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            noise_patterns = ['å£ã‚³ãƒŸã®å•é¡Œã‚’å ±å‘Š', 'ãƒ­ã‚³ãƒŸã®å•é¡Œã‚’å ±å‘Š', 'æ¡ç”¨ã§', 'p)', 'å…¨ã¦ã®å£ã‚³ãƒŸã‚’èª­ã‚€', 'ã‚‚ã£ã¨è¦‹ã‚‹', 'ç—…é™¢æƒ…å ±', 'ç—…é™¢ãƒ»ç ”ä¿®']
            if any(noise in university for noise in noise_patterns):
                university = ''
            
            # â˜…è¿½åŠ 
            university = normalize_university_name(university)

            grade = last_year_match.group(2)
            gender = last_year_match.group(3) if last_year_match.group(3) else ''
            participation = last_year_match.group(4)
            year = last_year_match.group(5)
            
            # ã“ã®è¦‹å‡ºã—ã‹ã‚‰æ¬¡ã®è¦‹å‡ºã—ã¾ã§ï¼ˆã¾ãŸã¯çµ‚ç«¯ã¾ã§ï¼‰ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            content_start = heading.end()
            if i + 1 < len(headings):
                content_end = headings[i + 1].start()
            else:
                content_end = len(text)
            
            content = text[content_start:content_end]
            
            # ãƒã‚¤ã‚ºé™¤å»ï¼ˆè¡Œå˜ä½ + å ±å‘Šè¡Œä»¥é™ãƒˆãƒªãƒ  + è¦‹å‡ºã—é™¤å»ï¼‰
            content = clean_review_text(content)

            # ç©ºã§ãªã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ä¿å­˜
            if content:
                all_sections.append({
                    'university': university,
                    'grade': grade,
                    'gender': gender,
                    'participation': participation,
                    'year': year,
                    'heading_type': heading_type,
                    'content': content,
                    'page': page_idx,
                    'pos': heading.start(),
                })
    
    # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å€‹åˆ¥ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã—ã¦å‡ºåŠ›ï¼ˆè‰¯ã„ç‚¹ãƒ»æ°—ã«ãªã‚‹ç‚¹ã¯åˆ¥è¡Œï¼‰
    reviews = []
    
    for section in all_sections:
        # è‰¯ã„ç‚¹ã®ã¿ or æ°—ã«ãªã‚‹ç‚¹ã®ã¿ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã—ã¦ä½œæˆ
        if section['heading_type'] == 'è‰¯ã„ç‚¹':
            reviews.append({
                'university': section['university'],
                'grade': section['grade'],
                'gender': section['gender'],
                'participation': section['participation'],
                'year': section['year'],
                'good_points': section['content'],
                'concerns': '',
                'source': 'OCR',
                'page': section.get('page', 0),
                'pos': section.get('pos', 0),
            })
        elif section['heading_type'] == 'æ°—ã«ãªã‚‹ç‚¹':
            reviews.append({
                'university': section['university'],
                'grade': section['grade'],
                'gender': section['gender'],
                'participation': section['participation'],
                'year': section['year'],
                'good_points': '',
                'concerns': section['content'],
                'source': 'OCR'
            })
    
    # ===== ã“ã“ã‹ã‚‰å·®ã—æ›¿ãˆ =====

    # ã§ãã‚Œã°ãƒšãƒ¼ã‚¸é †ã«ä¸¦ã¹ã‚‹ï¼ˆpage/posã‚’å…¥ã‚Œã¦ãªã„ãªã‚‰ã“ã®sortã¯å®Ÿè³ªãã®ã¾ã¾ï¼‰
    reviews.sort(key=lambda r: (r.get('page', 0), r.get('pos', 0)))

    # 1) éš£åŒå£«ã®ã€Œé‡ãªã‚Šã€ã‚’çµåˆã—ã¦æ–­è£‚ã‚’ç›´ã™
    if MERGE_ADJACENT_OVERLAP:
        reviews = merge_adjacent_overlapping_reviews(reviews)

    # 2) ãã‚Œã§ã‚‚æ®‹ã‚‹ã€Œã»ã¼åŒã˜ã€é‡è¤‡ã‚’é™¤å»ï¼ˆé•·ã„æ–¹ã‚’æ®‹ã™ï¼‰
    final_reviews = []

    def same_meta(a, b):
        if a.get('year') != b.get('year'):
            return False
        if a.get('grade') != b.get('grade'):
            return False
        if a.get('participation') != b.get('participation'):
            return False

        ga = (a.get('gender') or '').strip()
        gb = (b.get('gender') or '').strip()
        if ga and gb and ga != gb:
            return False

        ua = (a.get('university') or '').strip()
        ub = (b.get('university') or '').strip()
        if ua and ub and ua != ub:
            return False

        return True

    def combined_norm(r):
        return re.sub(r'\s+', '', (r.get('good_points', '') + '\n' + r.get('concerns', '')).strip())

    for r in reviews:
        r_norm = combined_norm(r)
        if not r_norm:
            continue

        merged = False

        # ç”»é¢ã¾ãŸãæƒ³å®šãªã®ã§ã€Œç›´è¿‘æ•°ä»¶ã€ã ã‘è¦‹ã‚Œã°ååˆ†ï¼ˆèª¤çµåˆã‚‚æ¸›ã‚‹ï¼‰
        for idx in range(max(0, len(final_reviews) - 3), len(final_reviews)):
            prev = final_reviews[idx]
            if not same_meta(prev, r):
                continue

            p_norm = combined_norm(prev)

            # ç‰‡æ–¹ãŒã‚‚ã†ç‰‡æ–¹ã«å«ã¾ã‚Œã‚‹ â†’ é•·ã„æ–¹ã ã‘æ®‹ã™
            if p_norm in r_norm:
                final_reviews[idx] = r
                merged = True
                break
            if r_norm in p_norm:
                merged = True
                break

            # ã»ã¼åŒã˜ â†’ é•·ã„æ–¹
            if SequenceMatcher(None, p_norm, r_norm).ratio() >= HIGH_SIM_RATIO:
                if len(r_norm) > len(p_norm):
                    final_reviews[idx] = r
                merged = True
                break

        if not merged:
            final_reviews.append(r)

    return final_reviews

    # ===== ã“ã“ã¾ã§å·®ã—æ›¿ãˆ =====



def load_xml_extracted_data(file_path):
    """XMLæŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return [line.rstrip('\n') for line in f]

TABLE_ALIGN_RE = re.compile(r'^\s*\|\s*:?-{2,}.*\|\s*$', re.UNICODE)

def normalize_xml_line(line: str) -> str:
    if not line:
        return ''
    # <br> ã‚’æ”¹è¡Œã«
    line = re.sub(r'<br\s*/?>', '\n', line, flags=re.IGNORECASE)

    # Markdownè¡¨ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆè¡Œã¯æ¨ã¦ã‚‹
    if TABLE_ALIGN_RE.match(line):
        return ''

    # | ã§å§‹ã¾ã‚‹è¡¨è¡Œã¯ã€ãƒ‘ã‚¤ãƒ—ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ã—ã¦å¹³æ–‡åŒ–
    if line.lstrip().startswith('|'):
        line = line.strip().strip('|')
        line = re.sub(r'\s*\|\s*', ' ', line)

    return line.strip()

def parse_xml_reviews(lines):
    """XMLæŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æŠ½å‡ºï¼ˆè¦‹å‡ºã—ãªã—ï¼‰"""
    reviews = []

    # ç—…é™¢æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ‚äº†ã‚’è¦‹ã¤ã‘ã‚‹
    start_idx = 0
    for i, raw in enumerate(lines):
        line = normalize_xml_line(raw)
        if not line:
            continue
        if re.search(r'ä¿®æ­£ãƒ»è¿½åŠ ã™ã‚‹|å£ã‚³ãƒŸã‚’æ›¸ã', line):
            start_idx = i + 1
            break

    # å¹´åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¤§å­¦åã‚’å«ã‚€å¯èƒ½æ€§ï¼‰
    year_pattern = re.compile(
        r'^(?:([^\s]+(?:å¤§å­¦|åŒ»ç§‘å¤§å­¦|åŒ»å­¦éƒ¨|åŒ»ç§‘æ­¯ç§‘å¤§å­¦|ç§‘å­¦å¤§å­¦))\s+)?'
        r'(\d+)å¹´\s+(ç”·æ€§\s+|å¥³æ€§\s+)?'
        r'(è¦‹å­¦ã—ãŸ|ãƒãƒƒãƒã—ãŸ|ã‚ªãƒ³ãƒ©ã‚¤ãƒ³èª¬æ˜ä¼šã«å‚åŠ ã—ãŸ|èª¬æ˜ä¼šã«å‚åŠ ã—ãŸ)\s+'
        r'(\d{4})å¹´åº¦'
    )

    i = start_idx
    while i < len(lines):
        line = normalize_xml_line(lines[i])

        # â˜… è¿½åŠ ï¼šç©ºãªã‚‰æ¬¡ã¸ï¼ˆiã‚’é€²ã‚ãªã„ã¨è©°ã¾ã‚‹ã“ã¨ãŒã‚ã‚‹ï¼‰
        if not line:
            i += 1
            continue

        # å¹´åº¦è¡Œã‚’æ¤œå‡º
        match = year_pattern.match(line)
        if not match:
            i += 1
            continue

        university = match.group(1) if match.group(1) else ''
        university = normalize_university_name(university)
        grade = match.group(2) + 'å¹´'
        gender = match.group(3).strip() if match.group(3) else ''
        participation = match.group(4)
        year = match.group(5) + 'å¹´åº¦'

        content_lines = []

        # â˜…é‡è¦ï¼šåŒã˜è¡Œã«ã€Œå¹´åº¦ + æœ¬æ–‡ã€ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆã€æ®‹ã‚Šã‚’æœ¬æ–‡ã¨ã—ã¦æ‹¾ã†
        rest = line[match.end():].strip()
        if rest:
            # normalize_xml_lineã§ <br> ã‚’ \n ã«ã—ã¦ã„ã‚‹ãªã‚‰åˆ†å‰²ã—ã¦å…¥ã‚Œã‚‹
            for part in rest.split('\n'):
                part = part.strip()
                if part:
                    content_lines.append(part)

        i += 1

        # æ¬¡ã®å¹´åº¦è¡Œã¾ãŸã¯çµ‚ç«¯ã¾ã§æœ¬æ–‡ã‚’åé›†
        while i < len(lines):
            next_line = normalize_xml_line(lines[i])

            # â˜… è¿½åŠ ï¼šç©ºãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ï¼ˆiã‚’é€²ã‚ã¦æ¬¡ã¸ï¼‰
            if not next_line:
                i += 1
                continue

            # æ¬¡ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«åˆ°é”
            if year_pattern.match(next_line):
                break

            # ã€Œãƒ­ã‚³ãƒŸ/å£ã‚³ãƒŸã®å•é¡Œã‚’å ±å‘Šã€ã‚’è¦‹ã¤ã‘ãŸã‚‰ã€ãã®ãƒ¬ãƒ“ãƒ¥ãƒ¼æœ¬æ–‡ã¯ã“ã“ã§çµ‚äº†
            if REPORT_LINE_RE.search(next_line):
                i += 1
                # æ¬¡ã®å¹´åº¦è¡Œï¼ˆæ¬¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰ã¾ã§ã‚¹ã‚­ãƒƒãƒ—
                while i < len(lines):
                    probe = normalize_xml_line(lines[i])
                    if probe and year_pattern.match(probe):
                        break
                    i += 1
                break

            # ãã®ä»–ã®è»½ã„ãƒã‚¤ã‚ºã¯ã‚¹ã‚­ãƒƒãƒ—
            if any(x in next_line for x in [
                'å…¨ã¦ã®å£ã‚³ãƒŸã‚’èª­ã‚€', 'ç—…é™¢æƒ…å ±',
                'ç·åˆç‚¹', 'å­¦æ­´ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼', 'å¿™ã—ã•', 'ãƒã‚¤ãƒ', 'ãƒã‚¤ãƒ‘ãƒ¼', 'ç ”ä¿®ã‚¹ã‚¿ã‚¤ãƒ«',
                'ã‚‚ã£ã¨è¦‹ã‚‹'
            ]):
                i += 1
                continue

            # é€šå¸¸æœ¬æ–‡ã¨ã—ã¦è¿½åŠ ï¼ˆ\n ã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§åˆ†å‰²ã—ã¦å…¥ã‚Œã‚‹ã®ãŒå®‰å…¨ï¼‰
            for part in next_line.split('\n'):
                part = part.strip()
                if part:
                    content_lines.append(part)

            i += 1

        all_text = '\n'.join(content_lines)
        all_text = clean_review_text(all_text)  # æœ€å¾Œã«å…±é€šã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

        if all_text:
            reviews.append({
                'university': university,
                'grade': grade,
                'gender': gender,
                'participation': participation,
                'year': year,
                'text': all_text,
                'source': 'XML'
            })

    return reviews



def text_similarity(text1, text2):
    """2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆ0-1ï¼‰"""
    # ç©ºç™½ã¨æ”¹è¡Œã‚’æ­£è¦åŒ–
    text1 = re.sub(r'\s+', '', text1)
    text2 = re.sub(r'\s+', '', text2)
    
    return SequenceMatcher(None, text1, text2).ratio()


def split_xml_by_ocr_structure(xml_text, ocr_good, ocr_concern):
    """OCRã®è¦‹å‡ºã—æ§‹é€ ã‚’ä½¿ã£ã¦XMLãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ï¼ˆç‰‡å´ã®ã¿ã®å ´åˆã¯å…¨é‡å¯„ã›ã‚‹ï¼‰"""
    ocr_good_len = len(re.sub(r'\s+', '', ocr_good or ''))
    ocr_concern_len = len(re.sub(r'\s+', '', ocr_concern or ''))
    total_ocr_len = ocr_good_len + ocr_concern_len

    # OCRæƒ…å ±ãŒã‚¼ãƒ­ãªã‚‰åˆ†å‰²ä¸èƒ½ï¼šå…¨éƒ¨è‰¯ã„ç‚¹å´ã«å¯„ã›ã‚‹ï¼ˆå¾“æ¥æŒ™å‹•ï¼‰
    if total_ocr_len == 0:
        return {'good_points': xml_text, 'concerns': ''}

    # â˜…é‡è¦ï¼šç‰‡å´ã—ã‹ç„¡ã„ãªã‚‰åˆ†å‰²ã›ãšã€ãã®å´ã«å…¨é‡å¯„ã›ã‚‹
    if ocr_good_len == 0 and ocr_concern_len > 0:
        return {'good_points': '', 'concerns': xml_text}

    if ocr_concern_len == 0 and ocr_good_len > 0:
        return {'good_points': xml_text, 'concerns': ''}

    # ã“ã“ã‹ã‚‰å…ˆã¯ã€Œä¸¡æ–¹ã‚ã‚‹ã€å ´åˆã ã‘æ¯”ç‡åˆ†å‰²
    xml_len = len(re.sub(r'\s+', '', xml_text))
    good_ratio = ocr_good_len / total_ocr_len
    target_split = int(xml_len * good_ratio)

    paragraphs = [p.strip() for p in xml_text.split('\n') if p.strip()]
    if len(paragraphs) <= 1:
        # æ®µè½ãŒç„¡ã„/1å€‹ã—ã‹ãªã„å ´åˆã¯æ¯”ç‡åˆ†å‰²ã§ããªã„ã®ã§è‰¯ã„ç‚¹å´ã«å¯„ã›ã‚‹
        return {'good_points': xml_text, 'concerns': ''}

    cumulative_len = 0
    best_split_idx = 1
    min_diff = float('inf')

    for i, para in enumerate(paragraphs):
        cumulative_len += len(re.sub(r'\s+', '', para))
        diff = abs(cumulative_len - target_split)
        if diff < min_diff:
            min_diff = diff
            best_split_idx = i + 1

    return {
        'good_points': '\n'.join(paragraphs[:best_split_idx]),
        'concerns': '\n'.join(paragraphs[best_split_idx:])
    }



def merge_reviews(ocr_reviews, xml_reviews):
    """OCRãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨XMLãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ãƒãƒ¼ã‚¸ï¼ˆé«˜ç²¾åº¦ç‰ˆï¼‰"""
    merged = []
    matched_xml_indices = set()
    
    print(f"\nğŸ”— ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ãƒãƒƒãƒãƒ³ã‚°ä¸­...")
    print(f"   OCRãƒ¬ãƒ“ãƒ¥ãƒ¼: {len(ocr_reviews)}ä»¶")
    print(f"   XMLãƒ¬ãƒ“ãƒ¥ãƒ¼: {len(xml_reviews)}ä»¶")
    
    # OCRãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’åŸºæº–ã«ãƒãƒƒãƒãƒ³ã‚°
    for ocr_review in ocr_reviews:
        best_match = None
        best_similarity = 0.0
        best_xml_idx = -1
        
        # åŒã˜å¹´åº¦ãƒ»å‚åŠ å½¢æ…‹ã®XMLãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ¢ã™
        for xml_idx, xml_review in enumerate(xml_reviews):
            if xml_idx in matched_xml_indices:
                continue
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒä¸€è‡´ã™ã‚‹ã‹ç¢ºèªï¼ˆå¤§å­¦åã¯é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ã§åˆ¤æ–­ï¼‰
            if (ocr_review['grade'] == xml_review['grade'] and
                ocr_review['participation'] == xml_review['participation'] and
                ocr_review['year'] == xml_review['year']):
                
                # ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚’è¨ˆç®—
                combined_ocr = ocr_review['good_points'] + ocr_review['concerns']
                similarity = text_similarity(combined_ocr, xml_review['text'])
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = xml_review
                    best_xml_idx = xml_idx
        
        # ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’å‡¦ç†
        if best_match and best_similarity > 0.3:  # 30%ä»¥ä¸Šã®é¡ä¼¼åº¦
            matched_xml_indices.add(best_xml_idx)
            
            # OCRã®è¦‹å‡ºã—æ§‹é€ ã§XMLã®é«˜ç²¾åº¦ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
            split_result = split_xml_by_ocr_structure(
                best_match['text'],
                ocr_review['good_points'],
                ocr_review['concerns']
            )
            
            merged.append({
                'university': ocr_review['university'],
                'grade': ocr_review['grade'],
                'gender': ocr_review['gender'] or best_match['gender'],
                'participation': ocr_review['participation'],
                'year': ocr_review['year'],
                'good_points': split_result['good_points'],
                'concerns': split_result['concerns'],
                'source': 'OCR structure + XML text',
                'similarity': f"{best_similarity:.2%}"
            })
            print(f"   âœ“ ãƒãƒƒãƒ: {ocr_review['year']} {ocr_review['grade']} (é¡ä¼¼åº¦: {best_similarity:.2%}) [XMLé«˜ç²¾åº¦ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨]")
        else:
            # ãƒãƒƒãƒã—ãªã„å ´åˆã¯OCRã®ã¿ä½¿ç”¨
            merged.append({
                'university': ocr_review['university'],
                'grade': ocr_review['grade'],
                'gender': ocr_review['gender'],
                'participation': ocr_review['participation'],
                'year': ocr_review['year'],
                'good_points': ocr_review['good_points'],
                'concerns': ocr_review['concerns'],
                'source': 'OCR only',
                'similarity': 'N/A'
            })
            print(f"   âš  ãƒãƒƒãƒãªã—: {ocr_review['year']} {ocr_review['grade']} (OCRã®ã¿ä½¿ç”¨)")
    
    # â˜… ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯åˆ†é¡ã¯å“è³ªãŒä½ã„ãŸã‚é™¤å¤–
    # ãƒãƒƒãƒã—ãªã‹ã£ãŸXMLãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯è¿½åŠ ã—ãªã„
    # for xml_idx, xml_review in enumerate(xml_reviews):
    #     if xml_idx not in matched_xml_indices:
    #         # ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã§è‰¯ã„ç‚¹ãƒ»æ°—ã«ãªã‚‹ç‚¹ã‚’æ¨å®š
    #         split_result = split_by_heuristics(xml_review['text'])
    #         
    #         merged.append({
    #             'university': xml_review['university'],
    #             'grade': xml_review['grade'],
    #             'gender': xml_review['gender'],
    #             'participation': xml_review['participation'],
    #             'year': xml_review['year'],
    #             'good_points': split_result['good_points'],
    #             'concerns': split_result['concerns'],
    #             'source': 'XML (heuristic split)',
    #             'similarity': 'N/A'
    #         })
    #         print(f"   + è¿½åŠ : {xml_review['year']} {xml_review['grade']} (ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯åˆ†é¡)")
    
    return merged


def split_by_heuristics(text):
    explicit = split_by_explicit_headings(text)
    if explicit:
        return explicit
    """ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã§è‰¯ã„ç‚¹ãƒ»æ°—ã«ãªã‚‹ç‚¹ã‚’åˆ†é¡"""
    paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
    
    if not paragraphs:
        return {'good_points': '', 'concerns': ''}
    
    # æˆ¦ç•¥1: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒãƒ¼ã‚«ãƒ¼æ¤œå‡º
    negative_markers = [
        'ä¸€æ–¹ã§ã€', 'ä¸€æ–¹ã€', 'ä¸€æ–¹ã§', 'ã—ã‹ã—ã€', 'ã—ã‹ã—', 'ãŸã ã—ã€', 'ãŸã ã—',
        'ã—ã‹ã—ãªãŒã‚‰ã€', 'ã—ã‹ã—ãªãŒã‚‰', 'ãŸã ã€', 'ãŸã ', 'ã¾ãŸã€',
        'æ°—ã«ãªã‚‹ç‚¹ã¨ã—ã¦ã¯', 'ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã¨ã—ã¦ã¯', 'èª²é¡Œã‚‚', 'æ‡¸å¿µç‚¹ã¨ã—ã¦ã¯'
    ]
    
    for i, para in enumerate(paragraphs):
        for marker in negative_markers:
            if para.startswith(marker) or marker in para[:50]:
                return {
                    'good_points': '\n'.join(paragraphs[:i]),
                    'concerns': '\n'.join(paragraphs[i:])
                }
    
    # æˆ¦ç•¥2: ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•æ•°ã§åˆ†å‰²ï¼ˆ3ã¤ä»¥ä¸Šãªã‚‰ä¸­é–“ç‚¹ï¼‰
    if len(paragraphs) >= 3:
        split_point = len(paragraphs) // 2
        return {
            'good_points': '\n'.join(paragraphs[:split_point]),
            'concerns': '\n'.join(paragraphs[split_point:])
        }
    
    # æˆ¦ç•¥3: é•·æ–‡ãªã‚‰ä¸­é–“ã§åˆ†å‰²
    if len(text) > 2000:
        mid_point = len(text) // 2
        split_pos = text.rfind('\n', 0, mid_point)
        if split_pos > 0:
            return {
                'good_points': text[:split_pos].strip(),
                'concerns': text[split_pos:].strip()
            }
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ã¦è‰¯ã„ç‚¹
    return {
        'good_points': '\n'.join(paragraphs),
        'concerns': ''
    }


def create_excel(reviews, hospital_info, output_path):
    """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
    
    def clean_cell_value(value):
        """Excelã‚»ãƒ«ç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if not value or not isinstance(value, str):
            return value
        
        # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»ï¼ˆã‚¿ãƒ–ã€æ”¹è¡Œã€ã‚­ãƒ£ãƒªãƒƒã‚¸ãƒªã‚¿ãƒ¼ãƒ³ã¯ä¿æŒï¼‰
        import unicodedata
        cleaned = ''.join(char for char in value if unicodedata.category(char)[0] != 'C' or char in '\t\n\r')
        
        # = ã§å§‹ã¾ã‚‹æ–‡å­—åˆ—ã¯æ•°å¼ã¨èª¤èªã•ã‚Œã‚‹ã®ã§ ' ã‚’å…ˆé ­ã«è¿½åŠ 
        if cleaned.startswith('='):
            cleaned = "'" + cleaned
        
        # @ ã§å§‹ã¾ã‚‹æ–‡å­—åˆ—ã‚‚æ•°å¼ã¨èª¤èªã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
        if cleaned.startswith('@'):
            cleaned = "'" + cleaned
        
        # + ã‚„ - ã§å§‹ã¾ã‚‹å ´åˆã‚‚æ•°å¼ã¨èª¤èªã•ã‚Œã‚‹å¯èƒ½æ€§
        if cleaned.startswith(('+', '-')) and len(cleaned) > 1 and cleaned[1].isdigit():
            cleaned = "'" + cleaned
        
        return cleaned
    
    wb = Workbook()
    
    # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚·ãƒ¼ãƒˆ
    ws_reviews = wb.active
    ws_reviews.title = "Reviews"
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    headers = ['å¤§å­¦', 'å­¦å¹´', 'æ€§åˆ¥', 'å‚åŠ å½¢æ…‹', 'å¹´åº¦', 'è‰¯ã„ç‚¹', 'æ°—ã«ãªã‚‹ç‚¹', 'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹', 'é¡ä¼¼åº¦']
    ws_reviews.append(headers)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ«
    header_fill = PatternFill(start_color='4472C4', end_color='4472C4', fill_type='solid')
    header_font = Font(bold=True, color='FFFFFF')
    for cell in ws_reviews[1]:
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = Alignment(horizontal='center', vertical='center')
    
    # ãƒ‡ãƒ¼ã‚¿è¡Œ
    for review in reviews:
        good = clean_review_text(review.get('good_points', ''))
        conc = clean_review_text(review.get('concerns', ''))

        ws_reviews.append([
            clean_cell_value(normalize_university_name(review.get('university', ''))),
            clean_cell_value(review['grade']),
            clean_cell_value(review.get('gender', '')),
            clean_cell_value(review['participation']),
            clean_cell_value(review['year']),
            clean_cell_value(good),
            clean_cell_value(conc),
            clean_cell_value(review.get('source', '')),
            review.get('similarity', '')
        ])

    # åˆ—å¹…èª¿æ•´
    ws_reviews.column_dimensions['A'].width = 25
    ws_reviews.column_dimensions['B'].width = 10
    ws_reviews.column_dimensions['C'].width = 8
    ws_reviews.column_dimensions['D'].width = 20
    ws_reviews.column_dimensions['E'].width = 12
    ws_reviews.column_dimensions['F'].width = 80
    ws_reviews.column_dimensions['G'].width = 80
    ws_reviews.column_dimensions['H'].width = 25
    ws_reviews.column_dimensions['I'].width = 12
    
    # ãƒ†ã‚­ã‚¹ãƒˆæŠ˜ã‚Šè¿”ã—
    for row in ws_reviews.iter_rows(min_row=2, max_row=ws_reviews.max_row, min_col=6, max_col=7):
        for cell in row:
            cell.alignment = Alignment(wrap_text=True, vertical='top')
    
    # ç—…é™¢æƒ…å ±ã‚·ãƒ¼ãƒˆ
    ws_info = wb.create_sheet("Hospital Info")
    ws_info.append(['é …ç›®', 'å†…å®¹'])
    ws_info['A1'].fill = header_fill
    ws_info['A1'].font = header_font
    ws_info['B1'].fill = header_fill
    ws_info['B1'].font = header_font
    
    for key, value in hospital_info.items():
        ws_info.append([clean_cell_value(key), clean_cell_value(value)])
    
    ws_info.column_dimensions['A'].width = 30
    ws_info.column_dimensions['B'].width = 100
    
    # ã‚»ãƒ«ã®æ–‡å­—åˆ¶é™ã‚’ç¢ºèªã—ã¦é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ‡ã‚Šè©°ã‚ã‚‹
    for row in ws_reviews.iter_rows(min_row=2, max_row=ws_reviews.max_row):
        for cell in row:
            if cell.value and isinstance(cell.value, str) and len(cell.value) > 32000:
                cell.value = cell.value[:32000] + '...'
    
    # ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
    try:
        wb.save(output_path)
        print(f"âœ… Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    except Exception as e:
        print(f"âš  Excelä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        # ä»£æ›¿ãƒ•ã‚¡ã‚¤ãƒ«åã§ä¿å­˜ã‚’è©¦ã¿ã‚‹
        import os
        alt_path = output_path.replace('.xlsx', '_backup.xlsx')
        print(f"ğŸ“ ä»£æ›¿ãƒ‘ã‚¹ã§ä¿å­˜ã‚’è©¦ã¿ã¾ã™: {alt_path}")
        wb.save(alt_path)
        print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ: {alt_path}")


def main():
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•: python merge_ocr_xml_reviews.py <OCR_JSONL> <XML_TXT> [OUTPUT_FILE]")
        print("ä¾‹: python merge_ocr_xml_reviews.py ã•ã„ãŸã¾èµ¤åå­—_OCR.jsonl ã•ã„ãŸã¾èµ¤åå­—.txt [ã•ã„ãŸã¾èµ¤åå­—_merged.xlsx]")
        sys.exit(1)
    
    ocr_file = sys.argv[1]
    xml_file = sys.argv[2]
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆç¬¬3å¼•æ•°ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰
    if len(sys.argv) >= 4:
        output_file = sys.argv[3]
    else:
        base_name = re.sub(r'(_OCR)?\.jsonl$', '', ocr_file)
        base_name = re.sub(r'\.txt$', '', base_name)
        output_file = f"{base_name}_merged.xlsx"
    
    print(f"ğŸ“– OCRãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {ocr_file}")
    ocr_data = load_jsonl(ocr_file)
    print(f"   {len(ocr_data)}ãƒšãƒ¼ã‚¸åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    print(f"\nğŸ“– XMLæŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {xml_file}")
    xml_lines = load_xml_extracted_data(xml_file)
    print(f"   {len(xml_lines)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    print(f"\nğŸ¥ ç—…é™¢æƒ…å ±ã‚’æŠ½å‡ºä¸­...")
    hospital_info = extract_hospital_info_xml(xml_lines)
    print(f"   {len(hospital_info)}é …ç›®ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
    
    print(f"\nğŸ“ OCRãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è§£æä¸­...")
    ocr_reviews = parse_ocr_reviews(ocr_data)
    print(f"   {len(ocr_reviews)}ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
    good_count = sum(1 for r in ocr_reviews if r['good_points'])
    concern_count = sum(1 for r in ocr_reviews if r['concerns'])
    print(f"   è‰¯ã„ç‚¹ã‚ã‚Š: {good_count}/{len(ocr_reviews)}")
    print(f"   æ°—ã«ãªã‚‹ç‚¹ã‚ã‚Š: {concern_count}/{len(ocr_reviews)}")
    
    print(f"\nğŸ“ XMLãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è§£æä¸­...")
    xml_reviews = parse_xml_reviews(xml_lines)
    print(f"   {len(xml_reviews)}ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
    
    # ãƒãƒ¼ã‚¸
    merged_reviews = merge_reviews(ocr_reviews, xml_reviews)
    
    print(f"\nğŸ“Š çµ±è¨ˆ:")
    print(f"   ç·ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°: {len(merged_reviews)}")
    print(f"   è‰¯ã„ç‚¹ã‚ã‚Š: {sum(1 for r in merged_reviews if r['good_points'])}/{len(merged_reviews)}")
    print(f"   æ°—ã«ãªã‚‹ç‚¹ã‚ã‚Š: {sum(1 for r in merged_reviews if r['concerns'])}/{len(merged_reviews)}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
    source_counts = {}
    for review in merged_reviews:
        source = review.get('source', 'unknown')
        source_counts[source] = source_counts.get(source, 0) + 1
    
    print(f"\nğŸ“Œ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥:")
    for source, count in sorted(source_counts.items()):
        print(f"   {source}: {count}ä»¶")
    
    print(f"\nğŸ’¡ ç²¾åº¦ã«ã¤ã„ã¦:")
    print(f"   OCR structure + XML text: OCRã®è¦‹å‡ºã—æ§‹é€  + XMLã®é«˜ç²¾åº¦æœ¬æ–‡")
    print(f"   OCR only: ç”»åƒèªè­˜ãƒ†ã‚­ã‚¹ãƒˆï¼ˆèª¤å­—ã®å¯èƒ½æ€§ã‚ã‚Šï¼‰")
    
    print(f"\nğŸ“Š Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­...")
    create_excel(merged_reviews, hospital_info, output_file)
    
    print(f"\nâœ¨ å®Œäº†!")


if __name__ == "__main__":
    main()
